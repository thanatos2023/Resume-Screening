{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resume NER.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fcjk7UqxO2af",
        "outputId": "f36154c0-b033-4b60-f79a-865e2e6bd1d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/archive.zip\n",
            "replace Entity Recognition in Resumes.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: no\n"
          ]
        }
      ],
      "source": [
        "!unzip '/content/archive.zip'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "3c1R6EJrO6LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "Hr4v3DajIfEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPPIFwAQIiHQ",
        "outputId": "61b8ff6d-8f7e-4415-9972-ec29bb66eaa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "ccj4t_WVOc0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/traindata.json','r') as f:\n",
        "  lines = f.readlines()"
      ],
      "metadata": {
        "id": "ndcC-e6_WLIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "# JSON formatting functions\n",
        "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
        "    training_data = []\n",
        "    lines=[]\n",
        "    with open(dataturks_JSON_FilePath, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "        data = json.loads(line)\n",
        "        text = data['content'].replace(\"\\n\", \" \")\n",
        "        entities = []\n",
        "        data_annotations = data['annotation']\n",
        "        if data_annotations is not None:\n",
        "            for annotation in data_annotations:\n",
        "                #only a single point in text annotation.\n",
        "                point = annotation['points'][0]\n",
        "                labels = annotation['label']\n",
        "                # handle both list of labels or a single label.\n",
        "                if not isinstance(labels, list):\n",
        "                    labels = [labels]\n",
        "\n",
        "                for label in labels:\n",
        "                    point_start = point['start']\n",
        "                    point_end = point['end']\n",
        "                    point_text = point['text']\n",
        "\n",
        "                    lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
        "                    rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
        "                    if lstrip_diff != 0:\n",
        "                        point_start = point_start + lstrip_diff\n",
        "                    if rstrip_diff != 0:\n",
        "                        point_end = point_end - rstrip_diff\n",
        "                    entities.append((point_start, point_end + 1 , label))\n",
        "        training_data.append((text, {\"entities\" : entities}))\n",
        "    return training_data"
      ],
      "metadata": {
        "id": "4zyYXeOdX0Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = convert_dataturks_to_spacy('/content/Entity Recognition in Resumes.json')"
      ],
      "metadata": {
        "id": "CJjp5nckOvlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data[0][1]"
      ],
      "metadata": {
        "id": "xyksJ_I8OzgK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7898c993-2292-4c61-b23b-20af594a0b8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'entities': [(1296, 1622, 'Skills'),\n",
              "  (993, 1154, 'Skills'),\n",
              "  (939, 957, 'College Name'),\n",
              "  (883, 905, 'College Name'),\n",
              "  (856, 860, 'Graduation Year'),\n",
              "  (771, 814, 'College Name'),\n",
              "  (727, 769, 'Designation'),\n",
              "  (407, 416, 'Companies worked at'),\n",
              "  (372, 405, 'Designation'),\n",
              "  (95, 145, 'Email Address'),\n",
              "  (60, 69, 'Location'),\n",
              "  (49, 58, 'Companies worked at'),\n",
              "  (13, 46, 'Designation'),\n",
              "  (0, 12, 'Name')]}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import random"
      ],
      "metadata": {
        "id": "I0wuPgggzIRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank('en')\n",
        "\n",
        "def train_model(train_data):\n",
        "  if 'ner' not in nlp.pipe_names:\n",
        "    ner = nlp.create_pipe('ner')\n",
        "    nlp.add_pipe(ner,last = True)\n",
        "\n",
        "    for text,annotation in training_data:\n",
        "      for ent in annotation['entities']:\n",
        "        ner.add_label(ent[2])\n",
        "\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "      optimizer = nlp.begin_training()\n",
        "      for itn in range(10):\n",
        "        print('Starting iterations'+str(itn))\n",
        "        random.shuffle(training_data)\n",
        "        loss = {}\n",
        "        index=0\n",
        "        for text,annotation in training_data:\n",
        "          try:\n",
        "            nlp.update(\n",
        "                [text],\n",
        "                [annotation],\n",
        "                drop=0.2,\n",
        "                sgd = optimizer,\n",
        "                losses=loss)\n",
        "          except Exception as e:\n",
        "            pass\n",
        "        print(loss)\n",
        "  \n",
        "            "
      ],
      "metadata": {
        "id": "sMO3fLe0zp-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(training_data)"
      ],
      "metadata": {
        "id": "VKK2YeKxz0Q1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f7356f8-4d5c-410e-9916-d1c09f22145a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting iterations0\n",
            "{'ner': 20140.110715183117}\n",
            "Starting iterations1\n",
            "{'ner': 16555.960930897283}\n",
            "Starting iterations2\n",
            "{'ner': 12217.728864604438}\n",
            "Starting iterations3\n",
            "{'ner': 9467.625958431092}\n",
            "Starting iterations4\n",
            "{'ner': 7719.760303009499}\n",
            "Starting iterations5\n",
            "{'ner': 7649.206090428478}\n",
            "Starting iterations6\n",
            "{'ner': 7230.383627521222}\n",
            "Starting iterations7\n",
            "{'ner': 6122.915956017891}\n",
            "Starting iterations8\n",
            "{'ner': 6173.087240941312}\n",
            "Starting iterations9\n",
            "{'ner': 5858.423052201798}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.to_disk('nlp_model')"
      ],
      "metadata": {
        "id": "ccmZQi_A_uOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_model = spacy.load('nlp_model')"
      ],
      "metadata": {
        "id": "NDllBqU6CbOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp_model(training_data[0][0])\n",
        "for ent in doc.ents:\n",
        "  print(ent.label_ + ' - ' + ent.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e04THAgCwgO",
        "outputId": "7a999fcd-c443-43d6-f6a6-29fdbda50e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name - Vishwanath P\n",
            "Designation - Senior Executive\n",
            "Companies worked at - MIS & Audit) - Job Profile in LabourNet Services India Pvt Ltd\n",
            "Location - Bengaluru\n",
            "Email Address - indeed.com/r/Vishwanath-P/06a16ac2d087d3c9\n",
            "Designation - Senior Executive\n",
            "Companies worked at - ANI Technologies\n",
            "Graduation Year - 2016\n",
            "Designation - HR Executive\n",
            "Companies worked at - Accenture\n",
            "Companies worked at - INFOSYS\n",
            "Companies worked at - Associate\n",
            "Companies worked at - Infosys\n",
            "Companies worked at - Infosys\n",
            "Degree - BSc\n",
            "College Name - V V Pura College of Science\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Znykf6IAESbA",
        "outputId": "7c4f2522-6c9f-45f8-940d-c29625f06713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.19.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 7.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.19.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extractor(resume):\n",
        "  import fitz\n",
        "  res = fitz.open(resume)\n",
        "  text=\"\"\n",
        "  for page_num in range(res.page_count):\n",
        "    obj = res[page_num]\n",
        "    text += obj.getText('text')\n",
        "    return text"
      ],
      "metadata": {
        "id": "0jAwh4b_FU9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume = extractor('/content/Karan Katariya_PI032_NITJSR.pdf')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgLSTsarFhuW",
        "outputId": "910eab76-4117-4af3-805f-3fc18ba8921d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Deprecation: 'getText' removed from class 'Page' after v1.19 - use 'get_text'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanResume(resumeText):\n",
        "    resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc\n",
        "    resumeText = re.sub('#\\S+', '', resumeText)  # remove hashtags\n",
        "    resumeText = re.sub('\\n', '  ', resumeText)  # remove mentions\n",
        "    return resumeText"
      ],
      "metadata": {
        "id": "V323pHL-FuA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume = cleanResume(resume)"
      ],
      "metadata": {
        "id": "oBaVA5KFF2Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp_model(resume)\n",
        "for ent in doc.ents:\n",
        "  print(ent.label_ + ' - ' + ent.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZdqsqABGgui",
        "outputId": "d7e31b7f-0d11-42f0-db8e-402a92165816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name - Karan Katariya\n",
            "Degree - B.tech in Production and Industrial\n",
            "College Name - National Institute of Technology, Jamshedpur\n",
            "Graduation Year - 2018\n",
            "Designation - Data Science And Analyst Intern\n",
            "Companies worked at - An\n",
            "Skills - Python   Data Analysis   Data Visualization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = extractor('/content/candidate_004.pdf')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwSk4mumGheb",
        "outputId": "5a4e2ac1-f836-4671-a2c7-9f67ee06a50a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Deprecation: 'getText' removed from class 'Page' after v1.19 - use 'get_text'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = nlp_model(test)\n",
        "for ent in doc1.ents:\n",
        "  print(ent.label_ + ' - ' + ent.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KlEwrvZJurp",
        "outputId": "b531741d-12e5-4902-8ea0-4de31b4975f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name - OLIVIA SANTOS\n",
            "Location - Cognizant\n",
            "Companies worked at - 2019\n",
            "Software Developer Analyst\n",
            "Worked as a software developer and analyst for various\n",
            "projects concerned with middleware. Actively participated\n",
            "in the development cycle and end to end needs of the\n",
            "projects.\n",
            "ACADEMIC PROFILE\n",
            "B.tech Electrical Engineering, 2018\n",
            "Bhubaneshwar Institute of Technology\n",
            "EXECUTIVE\n",
            "SUMMARY\n",
            "Data Engineer and solution\n",
            "specialist. Focused on using data\n",
            "building and data driven\n",
            "production ready procedures.\n",
            "PERSONAL\n",
            "SKILLS\n",
            "Machine Learning, Data Analytics,\n",
            "Project Management, Software\n",
            "Development, Agile Methodology,\n",
            "Business Growth.\n",
            "EXTRA-CURRICULARS\n",
            "EXTRA-CURRICULARS\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tv0GVZT1J1E_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}